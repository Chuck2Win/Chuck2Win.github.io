---
layout: post
title:  "RoBERTa"
description: RoBERTa 정리
date:   2021-04-14 
categories: contextual representation 
---



A Robustly Optimized BERT Pretraining Approach

`key hyperparmeters` & `training data size`

★ 1. More data, Bigger Batch size, Train Longer

★ 2. `Remove NSP objectives`

★ 3. Training Longer Sequences

★ 4. `Dynamic Masking`

`개인적으로 NSP를 제거하고 Dynamic Masking을 한 것이 의미가 있다고 생각함.`

## Contributions

- present a set of important `BERT design choices` and `training strategies` and introduce alternatives that lead to better downstream task performance
- `novel dataset` (CC News) and using more data for pretraining improves performance on down stream tasks
- mask language model is still competitive!

---

## Review 

## BERT

1. NSP

   [CLS] x1,...,xN [SEP] y1,...,yM [EOS] (이 길이를 512로 제한했었음)

   ✔ 이 때 `segment`s usually consist of more than one natural sentence - 기존까진 하나의 문장으로 인지했는데, 그것이 아니였군.

2. Objectives

   - **MLM** - cross entropy

     sequence에서 15%정도를 [MASK]를 씌움(여기서 80%는 [MASK], 10%는 random, 10%는 그대로)

     

   - **NSP** - binary classification

     positive - segment1, segment2가 consecutive sentences임

     negative - segment1, segment2가 <u>다른 document</u>에서 sampling  됬음.

3. Optimization

   Adam($\beta1=0.9$, $\beta2=0.999$, $\epsilon = 1e-6$)

   L2 decay(0.01)

   lr $1e-4$ (warmup 10,000 step, linearly decay)

   dropout  $1e-1$

   GeLU activation

   steps $1,000,000$ 

   Batch $256$

   Maximum length $512$

4. Data

   BOOKCORPUS, WIKIPEDIA

## RoBERTa

1. Data

   BOOKCORPUS, WIKIPIDEA, CC-NEWS, OPENWEBTEXT, STORIES

2. Evaluation

   GLUE, SQuAD(`extractive answering`), RACE

3. Model

   BERT_base (L=12, H=768, A=12, 110M params) - Layer : 12, d_model : 768, n_head : 12

### Static vs Dynamic Masking

원래 BERT에서는 static masking을 함(즉, 데이터를 보면 해당 문장에 masking하고 저장하는 형태)

그러나 RoBERTa에서는 비교를 위해(mask된 부분을 덜 보게끔 하기 위해) masking된 문장을 10개의 set를 만들었고, dynamic masking된 문장은 1개만(그냥 dataloader로 들어갈때 그때 masking한드아)

- masking된 문장은 고로 epoch이 40이면 같은 mask는 4번만 보게끔 되는 것이고 dynamic은 할때마다 바뀌니 늘 새롭지

결과가 엄청 차이가 나진 않지만, 그래도 필자들은 `dynamic masking`을 쓴다고 함

![table1](https://github.com/Chuck2Win/Chuck2Win.github.io/blob/master/img/RoBERTa/table1.png)



### Next Sentence Prediction

next sentence prediction은 document segments를 concat해서 해당 segment가 같은 document에서 온 것인지 아니면, 다른 document에서 온 것인지를 익히고자 하는 것임.(이를 next sentence prediction으로 하게 됨. <u>즉 궁극적인 목표를 살짝 바꿔서 다음에 올 문장이 진짜 다음에서 온 것인지(same document) 아니면 아닌지로 학습하게 함</u>)

이에 대한 실요성 문제가 있어서.. 실험을 진행해봄

이 부분이 잘 이해가 안갔었음.

|                     |                                                              |
| ------------------- | ------------------------------------------------------------ |
| Segment-pair + NSP  | 각 segment는 1개 이상의 sentence를 갖고 있음. 총 token의 길이는 512 이하 |
| Sentence-pair + NSP | 1개의 sentence만을 가지고 있음(연속된다면 같은 문서에서, 아니라면 다른 문서에서 sampling) |
| Full Sentences      |                                                              |
| Doc Sentences       |                                                              |

- Full Sentences

  contigous한 sentences를 하나 또는 그 이상의 문서에서 sampling함. 그래서 512로 맞춘다.

  만약 하나의 문서의 끝부분이라면, 다음 문서에서 sentence를 sampling해서 붙인다(사이에는 extra separator를 추가). NSP Loss를 제거

- Doc Sentences

  Full sentences와 비슷하나, `하나의 문서`로만 국한.

![table2](https://github.com/Chuck2Win/Chuck2Win.github.io/blob/master/img/RoBERTa/table2.png)

- NSP Loss를 쓸 경우, Segment pair이 sentence pair보다 성능이 좋다(sentence를 쓸 경우 long range dependencies를 못익힌다고 주장함)
- 그리고 NSP Loss가 없는 경우가 더 성능이 우수해짐(단순히 기존 BERT Model에서 NSP Loss만 빼도 된다곤 말함)
- Doc Sentences의 경우 마지막 부에선 512보다 짧아지기 때문에 dynamic하게 batch size를 변경하기 때문에(batchsize*tokens의 수를 일정하게 하기 위함 같음) 그래서 그냥 Full Sentences를 쓴다고 함.

? 근데, 기존 BERT Model에서 NSP Loss를 빼고 진행해도 된다고 했는데, Negative하더라도 그냥 쓰는 것인가?

- https://datascience.stackexchange.com/questions/76872/next-sentence-prediction-in-roberta

  친절한 설명. 즉 4개 중에서 처음 두개는 NSP를 실시한 것이고(다만, segment의 크기로 비교한 것), 뒤에 두 개는 NSP를 없앴다(BERT의 것으로 생각하면, `Positive만 존재`하는 것임. 물론 cross document의 경우 추가적인 sep token을 추가하는 것임)

### Batch size

기존 BERT는 1M step * 256 Bs ( = 125K step * 2K Bs = 31K step * 8K Bs)  (대략 2억 5천으로 유사)

- 이래야 computational cost가 유사

![table3](https://github.com/Chuck2Win/Chuck2Win.github.io/blob/master/img/RoBERTa/table3.png)

- lr은 각각의 것마다 tunning을 진행했음.



ppl은 MLM에서의 ppl인데, 보게 되면 궁극적으로 bs가 클수록 성능이 좋아진다고 한다(down stream task역시도) - 물론 여기선 2K가 가장 컸지만.. 

그리고 큰 batch size일수록 병렬화가 쉽다고 이야기함.

### Text Encoding

BPE encoding을 활용함.



## Result

![table4](https://github.com/Chuck2Win/Chuck2Win.github.io/blob/master/img/RoBERTa/table4.png)

더 학습을 시킨다고 overfit이 되진 않은 점이 인상적임.

